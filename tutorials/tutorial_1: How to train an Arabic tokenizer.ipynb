{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[HuggingFace] Arabic_SubWordtokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWBMdkFAPc9w"
      },
      "source": [
        "## Tutorial 1: Arabic Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "هذا الملف هو محاولة لتبسيط بعض المفاهيم المتعلقة بالمعالجة اللغوية للمهتمين من العرب حيث نحاول تطبيق بعض مفاهيم البرمجة اللغوية على البيانات العربية وشرح الخوارزميات والطرق المتعلقة بذلك.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- This series of tutorials intends to explain how to leverage HuggingFace tools for arabic natural language processing. This is the first tutorial which emphsis on the use of tokenizers from HuggingFace. \n",
        "\n",
        "- Most of the notebook is written by The great team of HuggingFace. However, I made a slight modification to adjust some expermint and ideas that emphsis on the arabic natural language processing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ILFYbNJOwOJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "cellView": "form",
        "outputId": "7b085d86-828d-4a65-ed9e-c9d4fa65def4"
      },
      "source": [
        "#@title\n",
        "%%html\n",
        "<div style=\"background-color: pink;\">\n",
        "  This Notebook written based on the great work of Hugging Face and <a href=\"https://github.com/aditya-malte\">Aditya Malte</a> <br> in this <a href=\"https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\">Notebook</a> with a slight modification to test different tokenizer algorithm with the arabic corpus.. \n",
        "</div>\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"background-color: pink;\">\n",
              "  This Notebook written based on the great work of Hugging Face and <a href=\"https://github.com/aditya-malte\">Aditya Malte</a> <br> in this <a href=\"https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb\">Notebook</a> with a slight modification to test different tokenizer algorithm with the arabic corpus.. \n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cqVhl7ajo86"
      },
      "source": [
        "### Tokenization \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3QOy-IZRfXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "2f6fc58a-d2a4-415d-ab2e-1ee4c1716478"
      },
      "source": [
        "## First step is to install the tokenizers library from HuggingFace. As you can see it is an easy task with only one line !\n",
        "## الخطوة الأولى نقوم بعملية تثبيت لأحد المكتبات التي سوف نستخدمها وهي تابعة لـ\n",
        "## HuggingFace \n",
        "\n",
        "!pip install tokenizers==0.9.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers==0.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY6cih7_Qd3e"
      },
      "source": [
        "## The second step is to import all the nesccary libraries for the target task\n",
        "## الخطوة الثانية نقوم باستدعاء المكتبات التي سوف نستخدمها في هذا التمرين\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer,BertWordPieceTokenizer #,SentencePieceBPETokenizer,CharBPETokenizer\n",
        "from tokenizers.decoders import ByteLevel\n",
        "\n",
        "from tokenizers.processors import BertProcessing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8F9Se39QcRU"
      },
      "source": [
        "### Download an Arabic Corpus\n",
        "\n",
        "The corpus is download from the following website : [open parallel corpus\n",
        "](http://opus.nlpl.eu/)\n",
        "\n",
        "\n",
        "The corpus is part of the MultiUN corpus. The corpus is a collection of translated documents from the United Nations. For more information, look into these two papers:\n",
        "\n",
        "* Eisele, A. and Chen, Y., 2010, May. MultiUN: A Multilingual Corpus from United Nation Documents. In LREC.\n",
        "\n",
        "* J. Tiedemann, 2012, Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vl3JexRFfIbz"
      },
      "source": [
        "## This function downloaed the data and extract its path\n",
        "## هذه الدالة تقوم بتحميل البيانات المذكورة بالأعلى عن طريق موقعها ثم استخراج ملف البيانات العربية \n",
        "## بعد ذلك نقوم بعملية استخلاص مسار الملف حتى نقوم باستدعاءه والملف يحفظ في قوقل درايف الخاص بك المرتبط في ايميل جيميل الذي تستخدمه في هذه الصفحة\n",
        "\n",
        "def download_Dataset():\n",
        "  \"\"\"Download the dataset and extract the path\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    paths\n",
        "        a string that holds the path of the dataset\n",
        "    \"\"\"\n",
        "  ## بداية نقوم بتحميل البيانات من الموقع \n",
        "  ## First we download the dataset from OPUS website\n",
        "  !wget https://object.pouta.csc.fi/OPUS-MultiUN/v1/mono/ar.txt.gz\n",
        "\n",
        "  ## بعد ذلك نقوم باستخراج البيانات\n",
        "  ## Then, we extract the dataset \n",
        "  !gzip -d /content/ar.txt.gz\n",
        "  \n",
        "  ## الخطوة الأخيرة هي القيام بإستخراج المسار الخاص بالبيانات \n",
        "  ## Finally we save the path of the extracted dataset by looking into the content folder \n",
        "  ## to find any file with .txt extention. \n",
        "  ## Important note: if you have more than one dataset or files with .txt extenstion\n",
        "  ## this process will save the paths for both files in the paths list\n",
        "  paths = [str(x) for x in Path(\"/content/\").glob(\"**/*.txt\")]\n",
        "  return paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h27sgj_afR3Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "974a37a5-77f4-424f-d2c0-67dc3f08acfd"
      },
      "source": [
        "## We call the function download_Dataset and save the return list in the paths\n",
        "## هذه الخطوة لإستدعاء الدالة السابقة والتي سوف تعيد لنا المسار الخاص بالملف على هيئة \n",
        "## LIST قائمة\n",
        "\n",
        "paths=download_Dataset()\n",
        "paths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-19 07:36:16--  https://object.pouta.csc.fi/OPUS-MultiUN/v1/mono/ar.txt.gz\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 615366163 (587M) [application/gzip]\n",
            "Saving to: ‘ar.txt.gz’\n",
            "\n",
            "ar.txt.gz           100%[===================>] 586.86M  23.6MB/s    in 26s     \n",
            "\n",
            "2020-10-19 07:36:43 (22.2 MB/s) - ‘ar.txt.gz’ saved [615366163/615366163]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/ar.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYbzkOp0eSfp"
      },
      "source": [
        "### Training Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82MKZG8xeRxD"
      },
      "source": [
        "\n",
        "def train_tokenizer(tokenizer,paths,strr=\"arabic\"):\n",
        "  \"\"\"train the tokenizer using huggingface tokenizers algorithm\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer : tokenizers.implementations\n",
        "        A tokenizer implementation that is not trained yet \n",
        "    paths : list\n",
        "        a list that holds the path of the dataset\n",
        "    strr : str, optional\n",
        "        a string name that used as base name for the saved dictionary \n",
        "    Returns\n",
        "    -------\n",
        "    tokenizer\n",
        "        a trained tokenizer \n",
        "    \"\"\"\n",
        "\n",
        "  ## A function that train the tokenizer using the given file with setup hyperparameters\n",
        "  ## The way of learning the tokenizaton is by looking into subword tokenization using \n",
        "  ## one of the implementation in the tokenizers library (ByteLevelBPETokenizer,BertWordPieceTokenizer, ... etc)\n",
        "  ## The tokenizer variable holds what is the target implementation \n",
        "\n",
        "  ## The tokenizer has some of the parameters that need to be filled or it will take the defult value if there is one.\n",
        "  ## file=paths : files:Union[str, List[str]] here we pass the path of the dataset that we will use to build our tokenizer.\n",
        "  ## vocab_size:int=30000 : the size of the vocabulary. Also, the tokenizer will stop when the vocabulary reach this number. \n",
        "  ## min_frequency:int=2 how many time a word (m) can exist in the vocabulary\n",
        "  ## special_tokens : here we tell the tokenizer about the notation of the special tokens. \n",
        "  tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "      \"<bos>\", \n",
        "      \"<pad>\",\n",
        "      \"<eos>\",\n",
        "      \"<unk>\",\n",
        "      \"<mask>\",\n",
        "  ])\n",
        "\n",
        "  # Save files to disk\n",
        "  tokenizer.save(\".\", strr )\n",
        "  \n",
        "  return tokenizer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFXSP_xLQ-fy"
      },
      "source": [
        "###Byte Level BPE Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRnQ14q6Q79D"
      },
      "source": [
        "# Initialize a Byete Level BPE tokenizer as introduced by OpenAI in the GPT2\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "# Start training the tokenizer over the corpus\n",
        "BPE_tokenizer=train_tokenizer(tokenizer,paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1Ur3l13Z290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f2205c0c-f7fb-4888-fe59-a6f848214855"
      },
      "source": [
        "\n",
        "BPE_tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"<bos>\", BPE_tokenizer.token_to_id(\"<bos>\")),\n",
        "    (\"<eos>\", BPE_tokenizer.token_to_id(\"<eos>\")),\n",
        ")\n",
        "BPE_tokenizer.enable_truncation(max_length=512)\n",
        "\n",
        "Decoder=ByteLevel()\n",
        "\n",
        "encoded_tokens= BPE_tokenizer.encode(\"سافر محمد وخالد إلى الرياض سويا\").tokens\n",
        "print(encoded_tokens)\n",
        "tokens=[]\n",
        "\n",
        "tokens=[ Decoder.decode(tok) for tok in encoded_tokens] \n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<eos>', 'Ø³', 'Ø§ÙģØ±', 'ĠÙħØŃÙħØ¯', 'ĠÙĪØ®', 'Ø§ÙĦØ¯', 'ĠØ¥ÙĦÙī', 'ĠØ§ÙĦØ±ÙĬØ§Ø¶', 'ĠØ³ÙĪÙĬØ§', '<bos>']\n",
            "['<eos>', 'س', 'افر', ' محمد', ' وخ', 'الد', ' إلى', ' الرياض', ' سويا', '<bos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x3AVQmrSCPg"
      },
      "source": [
        "### Bert Word Piece Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y4ozpiZSFtw"
      },
      "source": [
        "# Initialize a tokenizer\n",
        "Bert_tokenizer = BertWordPieceTokenizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBI3Wqg6bKF9"
      },
      "source": [
        "BertTokenizer=train_tokenizer(Bert_tokenizer,paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q145tkoGbRy9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b6b4fb2c-b1b5-4e6f-ce18-d3e183805702"
      },
      "source": [
        "BertTokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"<bos>\", BertTokenizer.token_to_id(\"<bos>\")),\n",
        "    (\"<eos>\", BertTokenizer.token_to_id(\"<eos>\")),\n",
        ")\n",
        "BertTokenizer.enable_truncation(max_length=512)\n",
        "\n",
        "print( BertTokenizer.encode(\"سافر محمد وخالد إلى الشرقية سويا\").tokens )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<eos>', 'سافر', 'محمد', 'وخال', '##د', 'الى', 'الشرقية', 'سويا', '<bos>']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
