{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adding a new dataset ( Arabic ) using Datasets library",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76deb982e470471b92c54a3e1c52a065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7bff86050f7c4aefa52717521aa24121",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1052fa5fa3949d485792eed567c1443",
              "IPY_MODEL_1f33a73446f84c29ace7fb48519631d0"
            ]
          }
        },
        "7bff86050f7c4aefa52717521aa24121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1052fa5fa3949d485792eed567c1443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_01475ed223b34165bf388b2a2fbe620b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3087,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3087,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29951ab34e4a4736ac8f15d813e16a9e"
          }
        },
        "1f33a73446f84c29ace7fb48519631d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b01bc00452042f4b84367a624b0016a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3.09k/3.09k [00:00&lt;00:00, 75.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c8611cf20b74dca8639c2d3bbc083a7"
          }
        },
        "01475ed223b34165bf388b2a2fbe620b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29951ab34e4a4736ac8f15d813e16a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b01bc00452042f4b84367a624b0016a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c8611cf20b74dca8639c2d3bbc083a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_puxWV42v08a"
      },
      "source": [
        "## Tutorial : Adding a new dataset ( Arabic as an example ):\n",
        "\n",
        "This tutorial aims to take you step by step to use Datasets [nlp previously] library from huggingface to add your own dataset by uploading it datasets CLI as user or organization. The library provides an easy and sharable access to the datasets. It, also, help the user from memory limitation in the RAM by smart memory mapping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCm4TIz61U6A"
      },
      "source": [
        "## Wikipidia dump corpus\n",
        "\n",
        "This corpus is used in this tutorial and it is originally downloaded from the following page : [Wiki_corpus](https://linguatools.org/tools/corpora/wikipedia-monolingual-corpora/)\n",
        "\n",
        "it is a corpus that extraced from wikipedia dumps. The arabic corpus consists from 131 M tokens and it is size is 3.3G. The file is xml extention. Therefore, we use perl language to parse XML document and extracts the text. \n",
        "\n",
        "Here's the papDer that explains how the corpus was constructed : \n",
        "\n",
        "*  D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
        "    *  In: Proceedings of the 8th International Language Ressources and Evaluation (LREC'12), 2012\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uco9j48Cvht7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1660b77-93af-440b-bca7-630a9dce2937"
      },
      "source": [
        "# install datasets \n",
        "!pip install datasets\n",
        "\n",
        "# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate the newest version\n",
        "import pyarrow\n",
        "if int(pyarrow.__version__.split('.')[1]) < 16:\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/38/0c24dce24767386123d528d27109024220db0e7a04467b658d587695241a/datasets-1.1.3-py3-none-any.whl (153kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 16.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 15.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 40kB 3.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 133kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 143kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.4)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e1/27958a70848f8f7089bff8d6ebe42519daf01f976d28b481e1bfd52c8097/pyarrow-2.0.0-cp36-cp36m-manylinux2014_x86_64.whl (17.7MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7MB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 48.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: pyarrow, xxhash, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.1.3 pyarrow-2.0.0 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk0xFj_bwuPc"
      },
      "source": [
        "import datasets\n",
        "import os; import psutil; import timeit\n",
        "from datasets import load_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di__f9Tzw4-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64917721-d11b-46f2-8eff-ac0fafd724d6"
      },
      "source": [
        "## git the datasets library because we are going to use datasets-cli python code\n",
        "!git clone https://github.com/huggingface/datasets.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'datasets'...\n",
            "remote: Enumerating objects: 17836, done.\u001b[K\n",
            "remote: Total 17836 (delta 0), reused 0 (delta 0), pack-reused 17836\u001b[K\n",
            "Receiving objects: 100% (17836/17836), 39.79 MiB | 32.91 MiB/s, done.\n",
            "Resolving deltas: 100% (7160/7160), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xIPqyGHyix0"
      },
      "source": [
        "## Test your dataset script\n",
        "\n",
        "We need first to write a dataset script that defines our dataset and its specifications. Here is an examples of dataset script written in the datasets library [Datsets_Script](https://github.com/huggingface/datasets/tree/master/datasets). It is important to test the written script. If the script is working properly, you can uopload it in huggingface website under your name. \n",
        "\n",
        "#### - How to test your script ?\n",
        "\n",
        "Create a folder for your dataset script under the path of datasets in datasets library folder like: (datasets/datasets/your_dataset_name). Then, create your dataset script code in the folder (your_dataset_name.py).\n",
        "|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrT8qboAxIfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "cellView": "form",
        "outputId": "719f5cbc-34c7-43e8-bd38-17c8616bf397"
      },
      "source": [
        "#@title\n",
        "%%html\n",
        "<div style=\"background-color: pink;\">\n",
        "  The dataset script used in this notebook is taken from the dataset script written to <a href=\"https://github.com/huggingface/nlp/tree/master/datasets/bookcorpus\">(bookcorpus)</a> script from huggingface/nlp with some modification due to the similiraity between both dataset.\n",
        "  Click (SHOW CODE) if you want to see the dataset script that used with WikiArabic Dataset.\n",
        "</div>\n",
        "\n",
        "\n",
        "\"\"\"\"\n",
        "This is the script I used for this dataset :\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2020 The TensorFlow Datasets Authors and the HuggingFace NLP Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "# Lint as: python3\n",
        "\"\"\" Wikipedia Arabic corpus.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "import nlp\n",
        "\n",
        "\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "The corpus is downloaded from the following page :https://linguatools.org/tools/corpora/wikipedia-monolingual-corpora/ \\\n",
        "it is a corpus that extraced from wikipedia dumps.\\\n",
        "The arabic corpus consists from 131 M tokens and it is size is 2.6G.\\\n",
        " The file is xml extention. \n",
        "Therefore, we use perl language to parse XML document and extracts the text.\\\n",
        "\"\"\"\n",
        "\n",
        "_CITATION = \"\"\"\\\n",
        "@inproceedings{goldhahn-etal-2012-building,\n",
        "    title = \"Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages\",\n",
        "    author = \"Goldhahn, Dirk  and\n",
        "      Eckart, Thomas  and\n",
        "      Quasthoff, Uwe\",\n",
        "    booktitle = \"Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)\",\n",
        "    month = may,\n",
        "    year = \"2012\",\n",
        "    address = \"Istanbul, Turkey\",\n",
        "    publisher = \"European Language Resources Association (ELRA)\",\n",
        "    url = \"http://www.lrec-conf.org/proceedings/lrec2012/pdf/327_Paper.pdf\",\n",
        "    pages = \"759--765\",\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "URL = \"https://drive.google.com/uc?export=download&id=1VqbVq2FDg8kVdeSm6EyyavRlb1JXYYos\"\n",
        "\n",
        "class WikiArabConfig(nlp.BuilderConfig)::\n",
        "    \"\"\"BuilderConfig for WikiArabic corpus.\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"BuilderConfig for WikiArabic.\n",
        "        Args:\n",
        "        **kwargs: keyword arguments forwarded to super.\n",
        "        \"\"\"\n",
        "        super(WikiArabConfig, self).__init__(\n",
        "            version=nlp.Version(\"1.0.0\", \"New split API (https://tensorflow.org/datasets/splits)\"), **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "class WikiArab(nlp.GeneratorBasedBuilder):\n",
        "    \"\"\"WikiArabic dataset.\"\"\"\n",
        "\n",
        "    BUILDER_CONFIGS = [WikiArabConfig(name=\"plain_text\", description=\"Plain text\",)]\n",
        "\n",
        "    def _info(self):\n",
        "        return nlp.DatasetInfo(\n",
        "            description=_DESCRIPTION,\n",
        "            features=nlp.Features({\"text\": nlp.Value(\"string\"),}),\n",
        "            supervised_keys=None,\n",
        "            homepage=\"https://linguatools.org/\",\n",
        "            citation=_CITATION,\n",
        "        )\n",
        "\n",
        "    def _vocab_text_gen(self, archive):\n",
        "        for _, ex in self._generate_examples(archive):\n",
        "            yield ex[\"text\"]\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        arch_path = dl_manager.download_and_extract(URL)\n",
        "\t\n",
        "        return [\n",
        "            nlp.SplitGenerator(name=nlp.Split.TRAIN, gen_kwargs={\"directory\": arch_path}),\n",
        "        ]\n",
        "\n",
        "    def _generate_examples(self, directory):\n",
        "        index=directory.rfind(\"datasets\")\n",
        "        index=index+8\n",
        "        url=directory[:index]\n",
        "        direct_name=directory[index+1:]\n",
        "        directory=url\n",
        "\n",
        "        files = [\n",
        "            os.path.join(directory, direct_name),\n",
        "        ]\n",
        "\n",
        "        _id = 0\n",
        "        for txt_file in files:\n",
        "            with open(txt_file, mode=\"r\") as f:\n",
        "                for line in f:\n",
        "                    yield _id, {\"text\": line.strip()}\n",
        "                    _id += 1\n",
        "\"\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div style=\"background-color: pink;\">\n",
              "  The dataset script used in this notebook is taken from the dataset script written to <a href=\"https://github.com/huggingface/nlp/tree/master/datasets/bookcorpus\">(bookcorpus)</a> script from huggingface/nlp with some modification due to the similiraity between both dataset.\n",
              "  Click (SHOW CODE) if you want to see the dataset script that used with WikiArabic Dataset.\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYFzEJ9X3u8p",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "20889227-c990-41c1-f8b8-1b2df042653d"
      },
      "source": [
        "## upload the dataset_script in your colab sesstion (It is important that your dataset script has the same name of the python class in the script )\n",
        "## The python script named (ar.py)\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "!ls\n",
        "\n",
        "## Make sure that the python file is in the correct path (/ar/ar.py)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-769524d3-0863-4c6b-89c6-cc5b975eb107\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-769524d3-0863-4c6b-89c6-cc5b975eb107\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ar.py to ar.py\n",
            "ar.py  datasets  drive\tnlp  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCNLoKATfgFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1af40070-0407-423a-e63e-2b43ed8e5140"
      },
      "source": [
        "## Test if the dataset script is working properly in terms of definning, downloading, and spliting the data/\n",
        "\n",
        "!python /content/datasets/datasets-cli test /content/ar/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-06 09:16:30.657453: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "Checking /content/ar/ar.py for additional imports.\n",
            "Lock 140611373389024 acquired on /content/ar/ar.py.lock\n",
            "Creating main folder for dataset /content/ar/ar.py at /root/.cache/huggingface/modules/datasets_modules/datasets/ar\n",
            "Creating specific version folder for dataset /content/ar/ar.py at /root/.cache/huggingface/modules/datasets_modules/datasets/ar/0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c\n",
            "Copying script file from /content/ar/ar.py to /root/.cache/huggingface/modules/datasets_modules/datasets/ar/0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c/ar.py\n",
            "Couldn't find dataset infos file at /content/ar/dataset_infos.json\n",
            "Creating metadata file for dataset /content/ar/ar.py at /root/.cache/huggingface/modules/datasets_modules/datasets/ar/0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c/ar.json\n",
            "Lock 140611373389024 released on /content/ar/ar.py.lock\n",
            "No config specified, defaulting to first: aracorpus/plain_text\n",
            "Lock 140611373389192 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_aracorpus_plain_text_1.0.0_0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c.lock\n",
            "Lock 140611373389192 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_aracorpus_plain_text_1.0.0_0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c.lock\n",
            "Testing builder 'plain_text' (1/1)\n",
            "Lock 140613903962408 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_aracorpus_plain_text_1.0.0_0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c.lock\n",
            "Generating dataset aracorpus (/root/.cache/huggingface/datasets/aracorpus/plain_text/1.0.0/0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c)\n",
            "Downloading and preparing dataset aracorpus/plain_text (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/aracorpus/plain_text/1.0.0/0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c...\n",
            "Lock 140613904412008 acquired on /root/.cache/huggingface/datasets/downloads/a679e5b058171943ba6f8e68e312e3a9a4c7938fd923db022cc16b99171523d3.lock\n",
            "https://object.pouta.csc.fi/OPUS-MultiUN/v1/mono/ar.txt.gz not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpulgdullu\n",
            "Downloading: 100% 615M/615M [01:39<00:00, 6.18MB/s]\n",
            "storing https://object.pouta.csc.fi/OPUS-MultiUN/v1/mono/ar.txt.gz in cache at /root/.cache/huggingface/datasets/downloads/a679e5b058171943ba6f8e68e312e3a9a4c7938fd923db022cc16b99171523d3\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/a679e5b058171943ba6f8e68e312e3a9a4c7938fd923db022cc16b99171523d3\n",
            "Lock 140613904412008 released on /root/.cache/huggingface/datasets/downloads/a679e5b058171943ba6f8e68e312e3a9a4c7938fd923db022cc16b99171523d3.lock\n",
            "Downloading took 1.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Lock 140613904412624 acquired on /root/.cache/huggingface/datasets/downloads/a679e5b058171943ba6f8e68e312e3a9a4c7938fd923db022cc16b99171523d3.lock\n",
            "Lock 140613904412624 released on /root/.cache/huggingface/datasets/downloads/a679e5b058171943ba6f8e68e312e3a9a4c7938fd923db022cc16b99171523d3.lock\n",
            "Unable to verify checksums.\n",
            "Generating split train\n",
            "Done writing 11119184 examples in 2694064013 bytes /root/.cache/huggingface/datasets/aracorpus/plain_text/1.0.0/0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c.incomplete/aracorpus-train.arrow.\n",
            "Unable to verify splits sizes.\n",
            "Lock 140613532395448 acquired on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_aracorpus_plain_text_1.0.0_0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c.incomplete.lock\n",
            "Lock 140613532395448 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_aracorpus_plain_text_1.0.0_0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c.incomplete.lock\n",
            "Dataset aracorpus downloaded and prepared to /root/.cache/huggingface/datasets/aracorpus/plain_text/1.0.0/0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c. Subsequent calls will reuse this data.\n",
            "Lock 140613903962408 released on /root/.cache/huggingface/datasets/_root_.cache_huggingface_datasets_aracorpus_plain_text_1.0.0_0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c.lock\n",
            "Constructing Dataset for split train, from /root/.cache/huggingface/datasets/aracorpus/plain_text/1.0.0/0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c\n",
            "100% 1/1 [00:00<00:00, 14.12it/s]\n",
            "Test successful.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib39rLgGU1Jh"
      },
      "source": [
        "## Upload your dataset script in HuggingFace website:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ked_2TdKh6Sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5697d6de-2ad9-46e6-83ce-d76238c9c7fd"
      },
      "source": [
        "## if the Test was successful, the next step is to download your script to under your name in HuggingFace server. Therefore, you need to create an account in HuggingFace : \n",
        "\n",
        "## After creating Account we login by the following command :\n",
        "\n",
        "!python /content/datasets/datasets-cli login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-06 09:30:32.442219: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        \n",
            "Username: k-halid\n",
            "Password: \n",
            "Login successful\n",
            "Your token: eQtxIOnQoAIHpzdrSvgPtyqXHiHFMwEuduQpvcyOLwYDRxrMcPgTQPzutEzFZHDKRzXLVfrCMrPmJPODBTknqkVpOjfeRffyYTZkFoyvVJWijXeIgBuvoJtaGsnHKwlF \n",
            "\n",
            "Your token has been saved to /root/.huggingface/token\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAMewVnss1sF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56663035-3929-4cd9-e283-d8735f5ceb94"
      },
      "source": [
        "!python /content/datasets/datasets-cli s3_datasets ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-06 09:30:58.032904: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "No shared file yet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEnlIV9tjEQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc378a8-72c5-408e-a69d-af23b8ea4ee4"
      },
      "source": [
        "## Use the following command to download your dataset script to your name in HuggingFace\n",
        "\n",
        "!python /content/datasets/datasets-cli upload_dataset /content/ar/\n",
        "\n",
        "### It will ask you if you want to save the file under the filename (ar/ar.py) or something else based on your dataset scrip name. Press Y to indicate your approval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-06 09:31:06.030623: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "About to upload file \u001b[1m/content/ar/ar.py\u001b[0m to S3 under filename \u001b[1mar/ar.py\u001b[0m and namespace \u001b[1mk-halid\u001b[0m\n",
            "About to upload file \u001b[1m/content/ar/ar.py.lock\u001b[0m to S3 under filename \u001b[1mar/ar.py.lock\u001b[0m and namespace \u001b[1mk-halid\u001b[0m\n",
            "Proceed? [Y/n] Y\n",
            "\u001b[1mUploading... This might take a while if files are large\u001b[0m\n",
            "Your file now lives at:\n",
            "https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/k-halid/ar/ar.py\n",
            "Your file now lives at:\n",
            "https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/k-halid/ar/ar.py.lock\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQIExtq9VJPL"
      },
      "source": [
        "## Download your datasets from HuggingFace website using Datasets library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osOq37gIs64E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "76deb982e470471b92c54a3e1c52a065",
            "7bff86050f7c4aefa52717521aa24121",
            "f1052fa5fa3949d485792eed567c1443",
            "1f33a73446f84c29ace7fb48519631d0",
            "01475ed223b34165bf388b2a2fbe620b",
            "29951ab34e4a4736ac8f15d813e16a9e",
            "8b01bc00452042f4b84367a624b0016a",
            "1c8611cf20b74dca8639c2d3bbc083a7"
          ]
        },
        "outputId": "24fe5026-1e31-48fd-f658-4d95759c9dfd"
      },
      "source": [
        "## Finally you can load the dataset by using your name (k-halid/) and including the name of the dataset script folder (/k-halid/ar)\n",
        "## Also, anyone can use the dataset if you give them the path of the dataset in huggingface website ('k-halid/ar')\n",
        "\n",
        "mem_before = psutil.Process(os.getpid()).memory_info().rss >> 20\n",
        "\n",
        "## If you upload your dataset under your name in HuggingFace replace \"k-halid/ar\" with \"your_account_name/dataset_script_folder_name\"\n",
        "wiki =  load_dataset('k-halid/ar', split='train')\n",
        "mem_after = psutil.Process(os.getpid()).memory_info().rss >> 20\n",
        "\n",
        "print(f\"RAM memory used: {(mem_after - mem_before)} MB\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76deb982e470471b92c54a3e1c52a065",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=3087.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset aracorpus (/root/.cache/huggingface/datasets/aracorpus/plain_text/1.0.0/0a3cb7b758363ce23a7007fe8328c685a987a71225f6cb283d894203de8a127c)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "RAM memory used: 73 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1Ts4CpFxrQe"
      },
      "source": [
        "## The dataset usually holds aproxmatly 3 GB space of RAM. \n",
        "## However, due to the efficient memory mapping of datasets library the data holds only 73 MB of the memory.\n",
        "## Next you can manipulate your data using datasets or transformers library from HuggingFace."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}